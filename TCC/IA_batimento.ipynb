{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzrsgbqMdUux",
        "outputId": "72b31938-3e6d-4bd7-c3f4-fa79e47856a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "32/32 [==============================] - 1s 11ms/step - loss: 1429.0272 - val_loss: 10.5804\n",
            "Epoch 2/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 18.2133 - val_loss: 11.3940\n",
            "Epoch 3/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5.6687 - val_loss: 10.7106\n",
            "Epoch 4/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.3658 - val_loss: 10.4162\n",
            "Epoch 5/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2700 - val_loss: 10.4272\n",
            "Epoch 6/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2987 - val_loss: 10.3887\n",
            "Epoch 7/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.3002 - val_loss: 10.3731\n",
            "Epoch 8/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.3943 - val_loss: 10.3199\n",
            "Epoch 9/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.3229 - val_loss: 10.2825\n",
            "Epoch 10/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5.3300 - val_loss: 10.2552\n",
            "Epoch 11/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2256 - val_loss: 10.2216\n",
            "Epoch 12/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.2369 - val_loss: 10.1832\n",
            "Epoch 13/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.2363 - val_loss: 10.1524\n",
            "Epoch 14/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2350 - val_loss: 10.1089\n",
            "Epoch 15/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 5.2651 - val_loss: 10.1280\n",
            "Epoch 16/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2262 - val_loss: 10.2969\n",
            "Epoch 17/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 5.1415 - val_loss: 10.0569\n",
            "Epoch 18/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.1217 - val_loss: 9.9656\n",
            "Epoch 19/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.1967 - val_loss: 9.9535\n",
            "Epoch 20/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1200 - val_loss: 9.9313\n",
            "Epoch 21/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.3987 - val_loss: 9.8247\n",
            "Epoch 22/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 5.1950 - val_loss: 9.9545\n",
            "Epoch 23/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 5.0336 - val_loss: 9.7370\n",
            "Epoch 24/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1261 - val_loss: 9.7479\n",
            "Epoch 25/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.9937 - val_loss: 9.6078\n",
            "Epoch 26/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 5.1702 - val_loss: 9.9568\n",
            "Epoch 27/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 5.1516 - val_loss: 9.4976\n",
            "Epoch 28/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.9431 - val_loss: 9.4762\n",
            "Epoch 29/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.9362 - val_loss: 9.6592\n",
            "Epoch 30/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.9567 - val_loss: 9.6189\n",
            "Epoch 31/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.8620 - val_loss: 9.5622\n",
            "Epoch 32/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.8269 - val_loss: 9.2407\n",
            "Epoch 33/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.8341 - val_loss: 9.2551\n",
            "Epoch 34/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.8919 - val_loss: 9.2683\n",
            "Epoch 35/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.9677 - val_loss: 9.2619\n",
            "Epoch 36/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.7652 - val_loss: 9.2400\n",
            "Epoch 37/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.8473 - val_loss: 9.2399\n",
            "Epoch 38/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.7367 - val_loss: 8.9107\n",
            "Epoch 39/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.6936 - val_loss: 8.8877\n",
            "Epoch 40/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.6456 - val_loss: 8.8058\n",
            "Epoch 41/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.7018 - val_loss: 9.9289\n",
            "Epoch 42/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.9582 - val_loss: 8.6780\n",
            "Epoch 43/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 4.6360 - val_loss: 8.6386\n",
            "Epoch 44/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.5442 - val_loss: 8.5548\n",
            "Epoch 45/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 4.6367 - val_loss: 8.2546\n",
            "Epoch 46/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.9240 - val_loss: 6.8981\n",
            "Epoch 47/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.6575 - val_loss: 6.3270\n",
            "Epoch 48/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.8107 - val_loss: 6.2328\n",
            "Epoch 49/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.4322 - val_loss: 6.1305\n",
            "Epoch 50/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3339 - val_loss: 6.1619\n",
            "Epoch 51/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.3492 - val_loss: 6.6575\n",
            "Epoch 52/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.5833 - val_loss: 5.9353\n",
            "Epoch 53/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.4338 - val_loss: 5.8292\n",
            "Epoch 54/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.2843 - val_loss: 6.0201\n",
            "Epoch 55/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.2824 - val_loss: 5.7276\n",
            "Epoch 56/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.2086 - val_loss: 5.6667\n",
            "Epoch 57/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1008 - val_loss: 5.4355\n",
            "Epoch 58/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.9966 - val_loss: 5.2201\n",
            "Epoch 59/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8846 - val_loss: 5.1220\n",
            "Epoch 60/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9400 - val_loss: 5.1173\n",
            "Epoch 61/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.9078 - val_loss: 5.0298\n",
            "Epoch 62/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 3.0926 - val_loss: 5.0161\n",
            "Epoch 63/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8827 - val_loss: 4.9796\n",
            "Epoch 64/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8043 - val_loss: 5.0051\n",
            "Epoch 65/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7971 - val_loss: 4.9816\n",
            "Epoch 66/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7230 - val_loss: 4.9261\n",
            "Epoch 67/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8588 - val_loss: 4.8770\n",
            "Epoch 68/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.7645 - val_loss: 4.8489\n",
            "Epoch 69/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7642 - val_loss: 4.7302\n",
            "Epoch 70/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6397 - val_loss: 4.7739\n",
            "Epoch 71/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.8916 - val_loss: 4.8468\n",
            "Epoch 72/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6875 - val_loss: 4.6179\n",
            "Epoch 73/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.6638 - val_loss: 5.2517\n",
            "Epoch 74/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.8052 - val_loss: 4.5525\n",
            "Epoch 75/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6394 - val_loss: 4.5632\n",
            "Epoch 76/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6552 - val_loss: 5.4909\n",
            "Epoch 77/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 3.1215 - val_loss: 4.5669\n",
            "Epoch 78/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5465 - val_loss: 4.4473\n",
            "Epoch 79/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.7863 - val_loss: 4.4977\n",
            "Epoch 80/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5775 - val_loss: 4.4552\n",
            "Epoch 81/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.7180 - val_loss: 4.8505\n",
            "Epoch 82/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.6638 - val_loss: 4.3882\n",
            "Epoch 83/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.5659 - val_loss: 4.6199\n",
            "Epoch 84/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.5371 - val_loss: 4.6749\n",
            "Epoch 85/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.5664 - val_loss: 4.3748\n",
            "Epoch 86/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4789 - val_loss: 4.2667\n",
            "Epoch 87/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.4554 - val_loss: 4.2070\n",
            "Epoch 88/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.4470 - val_loss: 4.1918\n",
            "Epoch 89/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.4862 - val_loss: 4.4395\n",
            "Epoch 90/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.4621 - val_loss: 4.2728\n",
            "Epoch 91/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.4622 - val_loss: 5.3782\n",
            "Epoch 92/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.6000 - val_loss: 4.3927\n",
            "Epoch 93/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.5230 - val_loss: 4.0845\n",
            "Epoch 94/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.4688 - val_loss: 4.3308\n",
            "Epoch 95/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.4254 - val_loss: 4.0687\n",
            "Epoch 96/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.3518 - val_loss: 4.0402\n",
            "Epoch 97/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.3135 - val_loss: 4.0017\n",
            "Epoch 98/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.5051 - val_loss: 3.9828\n",
            "Epoch 99/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.3479 - val_loss: 3.9847\n",
            "Epoch 100/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.3595 - val_loss: 3.9518\n",
            "Epoch 101/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.2679 - val_loss: 4.2108\n",
            "Epoch 102/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.4214 - val_loss: 4.1224\n",
            "Epoch 103/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4940 - val_loss: 4.0595\n",
            "Epoch 104/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3263 - val_loss: 3.9655\n",
            "Epoch 105/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3665 - val_loss: 3.9120\n",
            "Epoch 106/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2743 - val_loss: 3.8580\n",
            "Epoch 107/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3416 - val_loss: 3.8820\n",
            "Epoch 108/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2919 - val_loss: 3.9273\n",
            "Epoch 109/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2971 - val_loss: 3.9684\n",
            "Epoch 110/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2357 - val_loss: 3.7623\n",
            "Epoch 111/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2052 - val_loss: 3.7817\n",
            "Epoch 112/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3870 - val_loss: 3.8696\n",
            "Epoch 113/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2987 - val_loss: 3.7140\n",
            "Epoch 114/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2810 - val_loss: 3.9838\n",
            "Epoch 115/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.4179 - val_loss: 4.0853\n",
            "Epoch 116/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.3081 - val_loss: 3.6942\n",
            "Epoch 117/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1691 - val_loss: 3.7841\n",
            "Epoch 118/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2437 - val_loss: 5.4129\n",
            "Epoch 119/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.9736 - val_loss: 3.7173\n",
            "Epoch 120/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2174 - val_loss: 3.6256\n",
            "Epoch 121/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1267 - val_loss: 3.6603\n",
            "Epoch 122/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1343 - val_loss: 3.6850\n",
            "Epoch 123/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2216 - val_loss: 3.9600\n",
            "Epoch 124/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2236 - val_loss: 3.7101\n",
            "Epoch 125/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1387 - val_loss: 3.5738\n",
            "Epoch 126/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.0827 - val_loss: 3.5292\n",
            "Epoch 127/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1738 - val_loss: 3.5456\n",
            "Epoch 128/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2083 - val_loss: 4.1923\n",
            "Epoch 129/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2346 - val_loss: 3.5020\n",
            "Epoch 130/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1811 - val_loss: 3.4938\n",
            "Epoch 131/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3235 - val_loss: 3.4744\n",
            "Epoch 132/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3543 - val_loss: 3.8924\n",
            "Epoch 133/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1166 - val_loss: 3.4674\n",
            "Epoch 134/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0913 - val_loss: 3.4380\n",
            "Epoch 135/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1431 - val_loss: 3.4285\n",
            "Epoch 136/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2912 - val_loss: 3.6683\n",
            "Epoch 137/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0866 - val_loss: 3.6676\n",
            "Epoch 138/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.2776 - val_loss: 3.7290\n",
            "Epoch 139/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2447 - val_loss: 3.4683\n",
            "Epoch 140/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1521 - val_loss: 3.3765\n",
            "Epoch 141/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0907 - val_loss: 3.6966\n",
            "Epoch 142/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2784 - val_loss: 3.4810\n",
            "Epoch 143/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9899 - val_loss: 3.3230\n",
            "Epoch 144/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0296 - val_loss: 3.3327\n",
            "Epoch 145/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.3823 - val_loss: 3.4861\n",
            "Epoch 146/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1103 - val_loss: 3.5428\n",
            "Epoch 147/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1185 - val_loss: 3.5976\n",
            "Epoch 148/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.2316 - val_loss: 3.3903\n",
            "Epoch 149/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0776 - val_loss: 3.9363\n",
            "Epoch 150/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.1062 - val_loss: 3.2765\n",
            "Epoch 151/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.0386 - val_loss: 3.4137\n",
            "Epoch 152/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9420 - val_loss: 3.4523\n",
            "Epoch 153/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.0139 - val_loss: 3.2371\n",
            "Epoch 154/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9731 - val_loss: 3.2470\n",
            "Epoch 155/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 2.0825 - val_loss: 3.2717\n",
            "Epoch 156/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9953 - val_loss: 3.3883\n",
            "Epoch 157/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9206 - val_loss: 3.3976\n",
            "Epoch 158/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.0944 - val_loss: 3.1773\n",
            "Epoch 159/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.9084 - val_loss: 3.1832\n",
            "Epoch 160/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.0457 - val_loss: 3.3856\n",
            "Epoch 161/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.1948 - val_loss: 3.1645\n",
            "Epoch 162/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9736 - val_loss: 3.7479\n",
            "Epoch 163/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.9739 - val_loss: 3.1265\n",
            "Epoch 164/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9064 - val_loss: 3.7695\n",
            "Epoch 165/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9899 - val_loss: 3.1782\n",
            "Epoch 166/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8983 - val_loss: 3.2722\n",
            "Epoch 167/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9711 - val_loss: 3.2770\n",
            "Epoch 168/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 2.0010 - val_loss: 3.4416\n",
            "Epoch 169/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.1668 - val_loss: 3.0742\n",
            "Epoch 170/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.9266 - val_loss: 3.0932\n",
            "Epoch 171/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1.9133 - val_loss: 3.0777\n",
            "Epoch 172/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.8955 - val_loss: 3.1019\n",
            "Epoch 173/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1.9011 - val_loss: 3.0261\n",
            "Epoch 174/200\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.8810 - val_loss: 3.0914\n",
            "Epoch 175/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.8139 - val_loss: 3.0217\n",
            "Epoch 176/200\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 1.8631 - val_loss: 3.1398\n",
            "Epoch 177/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.9650 - val_loss: 2.9993\n",
            "Epoch 178/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.8554 - val_loss: 3.0084\n",
            "Epoch 179/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 1.8680 - val_loss: 2.9831\n",
            "Epoch 180/200\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 2.2162 - val_loss: 3.0105\n",
            "Epoch 181/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8202 - val_loss: 3.0879\n",
            "Epoch 182/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9352 - val_loss: 2.9471\n",
            "Epoch 183/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8992 - val_loss: 3.0782\n",
            "Epoch 184/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.8461 - val_loss: 3.0837\n",
            "Epoch 185/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.9348 - val_loss: 3.0670\n",
            "Epoch 186/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7523 - val_loss: 2.9299\n",
            "Epoch 187/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7459 - val_loss: 3.4736\n",
            "Epoch 188/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.8640 - val_loss: 3.2274\n",
            "Epoch 189/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9013 - val_loss: 2.9249\n",
            "Epoch 190/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7639 - val_loss: 3.1638\n",
            "Epoch 191/200\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 1.9481 - val_loss: 2.9617\n",
            "Epoch 192/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8251 - val_loss: 2.8971\n",
            "Epoch 193/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7684 - val_loss: 2.9299\n",
            "Epoch 194/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8715 - val_loss: 2.9098\n",
            "Epoch 195/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.7046 - val_loss: 2.8618\n",
            "Epoch 196/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 2.0432 - val_loss: 3.5102\n",
            "Epoch 197/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9765 - val_loss: 2.8471\n",
            "Epoch 198/200\n",
            "32/32 [==============================] - 0s 5ms/step - loss: 1.9177 - val_loss: 2.8410\n",
            "Epoch 199/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8341 - val_loss: 3.4993\n",
            "Epoch 200/200\n",
            "32/32 [==============================] - 0s 4ms/step - loss: 1.8920 - val_loss: 3.2692\n",
            "17/17 [==============================] - 0s 2ms/step\n",
            "Previsões dos próximos batimentos cardíacos (inteiros): [100 100 101 101 100 100  99  98  98  96  95  93  92  91  93  92  90  89\n",
            "  90  92  93  92  89  88  87  88  89  89  87  85  84  86  87  86  83  83\n",
            "  84  84  84  78  78  81  83  84  86  89  89  89  89  87  86  87  89  90\n",
            "  90  90  90  90  92  93  93  91  90  91  91  91  90  88  86  87  89  88\n",
            "  84  81  81  83  84  87  88  88  89  90  91  93  93  90  88  89  91  92\n",
            "  91  89  87  86  84  84  87  85  84  87  89  88  87  86  88  89  89  89\n",
            "  88  86  86  87  87  85  84  87  89  88  86  86  88  90  89  89  89  90\n",
            "  91  90  89  89  90  91  92  92  90  89  90  91  92  91  91  90  91  91\n",
            "  92  90  87  87  88  91  92  90  90  91  92  91  91  91  91  91  92  93\n",
            "  93  94  94  95  95  94  94  95  94  95  96  96  95  96  97  95  93  91\n",
            "  91  90  89  88  88  87  88  90  91  90  88  88  90  92  91  88  87  87\n",
            "  88  90  91  90  88  87  86  86  88  88  87  86  87  88  88  89  90  90\n",
            "  89  90  90  89  88  87  87  87  88  88  87  87  88  90  91  90  89  88\n",
            "  88  89  91  90  90  90  92  93  92  92  92  97  98  97  96  95  93  93\n",
            "  93  94  94  93  94  95  95  94  93  93  93  94  95  95  95  96  97  98\n",
            "  98  98  96  94  92  92  93  92  91  91  91  92  92  91  91  92  93  93\n",
            "  94  96  96  96  96  96  96  95  93  93  94  94  93  92  92  93  94  94\n",
            "  93  94  95  95  93  93  92  92  93  93  94  96  98  98  98 100 101 101\n",
            " 101 101  99  96  95  95  96  98  99 100 102 102 103 104 104 103 102  99\n",
            "  98 100 102 103 103 103 103 103 103 103 103 103  90  88  93  95  95  95\n",
            "  96  96  95  94  95  95  94  95  96  98  99  99 100 100 100 101 101 100\n",
            "  98  95  96  99  97  95  94  95  96  97  97  97  97  98  99 100  99  99\n",
            " 100  99  99 100  99  98  97  97  97  94  92  92  91  90  91  91  92  93\n",
            "  93  93  93  92  93  93  95  96  97  99 100 100 101 101 102 101  98  98\n",
            "  99  99  98  98  98  98  98  98  98  98  85  82  86  86  86  86  86  85\n",
            "  82  80  81  82  79  73  73  77  79  79  79  80  81  83  84  85  85  85\n",
            "  83  83  84  81  78  78  78  80  81  80  80  83  85  85  84  84  83  84\n",
            "  85  82  80  81  81  82  82  80  81  83  84  85  86  86  85  85  86  87\n",
            "  89  91  92  93  94  95  94  93  94  93  93  94  94  95  98 101 101 100\n",
            " 100 101 101 102 103 102 103 103 103 104 104 103 101  99  98]\n",
            "1/1 [==============================] - 0s 163ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Próximos 10 batimentos cardíacos previstos (inteiros): [91, 90, 89, 88, 88, 87, 87, 86, 85, 84]\n",
            "Média dos próximos 10 batimentos cardíacos previstos (inteiros): 88\n",
            "Média dos 10 novos batimentos cardíacos inseridos manualmente (inteiros): 107\n",
            "Aviso: O batimento cardíaco caiu muito.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "\n",
        "# 1. Carregar o dataset e manter apenas a primeira coluna\n",
        "dataset = pd.read_csv('heart_rate.csv')\n",
        "heartbeats = dataset.iloc[:, 0].values  # Mantendo apenas a primeira coluna\n",
        "\n",
        "# 2. Função para criar sequências de dados para o treinamento do modelo\n",
        "def create_sequences(data, seq_length):\n",
        "    xs = []\n",
        "    ys = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:i+seq_length]\n",
        "        y = data[i+seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "# 3. Definindo o tamanho das sequências\n",
        "seq_length = 10\n",
        "\n",
        "# 4. Criando sequências de dados\n",
        "X, y = create_sequences(heartbeats, seq_length)\n",
        "\n",
        "# 5. Dividindo os dados em treinamento (70%) e teste (30%)\n",
        "split_index = int(0.7 * len(X))\n",
        "X_train, X_test = X[:split_index], X[split_index:]\n",
        "y_train, y_test = y[:split_index], y[split_index:]\n",
        "\n",
        "# 6. Remodelando os dados para o formato esperado pelo RNN\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# 7. Criando o modelo RNN\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, activation='relu', input_shape=(seq_length, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 8. Compilando o modelo\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# 9. Treinando o modelo\n",
        "model.fit(X_train, y_train, epochs=200, validation_split=0.2, verbose=1)\n",
        "\n",
        "# 10. Fazendo previsões com o modelo treinado\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# 11. Arredondando as previsões para o inteiro mais próximo\n",
        "predictions = np.round(predictions).astype(int)\n",
        "\n",
        "# Exibindo algumas previsões\n",
        "print(\"Previsões dos próximos batimentos cardíacos (inteiros):\", predictions.flatten())\n",
        "\n",
        "# 12. Utilizando 15 novos batimentos para prever os próximos 10 batimentos\n",
        "new_heartbeats = np.array([89, 90, 91, 91, 92, 91, 91, 90, 92, 93, 88, 89, 90, 91, 92])\n",
        "next_heartbeats = []\n",
        "\n",
        "# Prever os próximos 10 batimentos\n",
        "for i in range(10):\n",
        "    input_seq = new_heartbeats[-seq_length:].reshape((1, seq_length, 1))\n",
        "    next_heartbeat = model.predict(input_seq)\n",
        "    next_heartbeat = np.round(next_heartbeat).astype(int)[0][0]\n",
        "    next_heartbeats.append(next_heartbeat)\n",
        "    new_heartbeats = np.append(new_heartbeats, next_heartbeat)\n",
        "\n",
        "print(\"Próximos 10 batimentos cardíacos previstos (inteiros):\", next_heartbeats)\n",
        "\n",
        "# 13. Calculando a média dos próximos 10 batimentos previstos\n",
        "mean_predicted = np.round(np.mean(next_heartbeats)).astype(int)\n",
        "print(\"Média dos próximos 10 batimentos cardíacos previstos (inteiros):\", mean_predicted)\n",
        "\n",
        "# 14. Inserindo manualmente 10 novos batimentos cardíacos\n",
        "manual_heartbeats = np.array([95, 99, 103, 105, 107, 107, 111, 114, 114, 113])\n",
        "mean_manual = np.round(np.mean(manual_heartbeats)).astype(int)\n",
        "print(\"Média dos 10 novos batimentos cardíacos inseridos manualmente (inteiros):\", mean_manual)\n",
        "\n",
        "# 15. Comparando as médias e emitindo alertas\n",
        "if mean_predicted > mean_manual + 5:\n",
        "    print(\"Alerta: O batimento cardíaco subiu muito em relação ao previsto.\")\n",
        "elif mean_predicted < mean_manual - 5:\n",
        "    print(\"Aviso: O batimento cardíaco caiu muito em relação ao previsto.\")\n",
        "else:\n",
        "    print(\"Os batimentos cardíacos estão dentro da faixa esperada.\")\n"
      ]
    }
  ]
}